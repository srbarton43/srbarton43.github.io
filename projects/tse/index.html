<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Tiny Search Engine | Samuel Barton</title>
<meta name=keywords content="search engine,software,c"><meta name=description content="A search engine written in C which crawls and caches webpages, indexes html attributes of each page, and queries a search string with page ranking"><meta name=author content="Sam Barton"><link rel=canonical href=https://srbarton43.github.io/projects/tse/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://srbarton43.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://srbarton43.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://srbarton43.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://srbarton43.github.io/apple-touch-icon.png><link rel=mask-icon href=https://srbarton43.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://srbarton43.github.io/projects/tse/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Tiny Search Engine"><meta property="og:description" content="A search engine written in C which crawls and caches webpages, indexes html attributes of each page, and queries a search string with page ranking"><meta property="og:type" content="article"><meta property="og:url" content="https://srbarton43.github.io/projects/tse/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2023-04-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-02T00:00:00+00:00"><meta property="og:site_name" content="Samuel Barton"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tiny Search Engine"><meta name=twitter:description content="A search engine written in C which crawls and caches webpages, indexes html attributes of each page, and queries a search string with page ranking"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://srbarton43.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Tiny Search Engine","item":"https://srbarton43.github.io/projects/tse/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Tiny Search Engine","name":"Tiny Search Engine","description":"A search engine written in C which crawls and caches webpages, indexes html attributes of each page, and queries a search string with page ranking","keywords":["search engine","software","c"],"articleBody":"Search Engine Architecture The architecture is based off of a 2001 paper (Searching the Web) by Arasu et al. published by the Association for Computing Machinery\nA schematic describing different components of the search engine design (from Arasu et al.)\nCrawler(s) and Crawl Control The crawler(s) browse(s) the web for the search engine, similar to how a human would follow links to reach different pages. The crawler starts at a specific root URL and starts the recursive process of extracting hyperlinks crawling these additional webpages. The crawler caches each retrieved page into a page repository. The crawler will continue visiting webpages until local resources are exhausted (i.e. no more storage in the page repository.\nIndexer Module The indexer module extracts all of the words from each page, and records the URL where the word occurred. This process creates a large lookup table which provides all of the URLs that point to pages where a given word occurs. Furthermore, the indexer also keeps track of the number of occurences per page, allowing for tracking pages with a large number of keywords on the single page.\nQuerier Engine Module This module is responsible for receiving and fulfilling search requests from users. It uses the page indexes to get a list of URLs which contain the keywords in the query. Then the module ranks pages based on the number of occurrences of keywords in the user query.\nImplementation See GitHub Repository for the source code.\nCrawler Implementation In this implementation, we use a single crawler to browse the webpages. The implementation is a standalone program which crawls the web starting from the seed url, fetches links from webpages continuing to a certain depth, and then caches these webpages in a specific page directory. The crawler ignores duplicate webpages when it scans each page for URLs. Furthermore, it has guardrails in-place to prevent it from crawling the entirety of the web (we don’t want to crawl pages that didn’t give us permission ;) )\nUsage:\n$ ./crawler seedURL pageDirectory maxDepth Here we see the verbose output of the program:\nThe Crawler in Action\nIndexer Implementation The indexer module produces an index file which stores the lookup table mapping each keyword to their host URLs with the number of ocurrences. It browses the page directory created by the crawler and uses this directory to construct the index. The module uses one main data structure: an index structure which maps from word to (docID, #occurences) pairs. The documentID is a unique ID for each webpage cached in the pageDirectory.\nUsage:\n./indexer pageDirectory indexFilename Querier Implementation The querier module is another standalone program which returns a page ranking according to a user’s query in stdin. To use the querier program, it simply requires the index file (created by the indexer program) and the page directory (created by the crawler program). It reads the index file into memory as an index data structure. The program parses each query and uses the union and intersection keywords (‘AND’ vs ‘OR’) in the query and uses the index to return a ranked list of URLs based on each score.\nUsage:\n./querier pageDirectory indexFilename Querier in Action\n","wordCount":"525","inLanguage":"en","datePublished":"2023-04-02T00:00:00Z","dateModified":"2023-04-02T00:00:00Z","author":{"@type":"Person","name":"Sam Barton"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://srbarton43.github.io/projects/tse/"},"publisher":{"@type":"Organization","name":"Samuel Barton","logo":{"@type":"ImageObject","url":"https://srbarton43.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://srbarton43.github.io/ accesskey=h title="Samuel Barton (Alt + H)">Samuel Barton</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://srbarton43.github.io/ title=About><span>About</span></a></li><li><a href=https://srbarton43.github.io/projects title=Projects><span>Projects</span></a></li><li><a href=https://srbarton43.github.io/writing title=Writing><span>Writing</span></a></li><li><a href=https://srbarton43.github.io/placeholder title=Resume/CV><span>Resume/CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Tiny Search Engine</h1><div class=post-description>A search engine written in C which crawls and caches webpages, indexes html attributes of each page, and queries a search string with page ranking</div><div class=post-meta><span title='2023-04-02 00:00:00 +0000 UTC'>April 2, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Sam Barton</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#search-engine-architecture aria-label="Search Engine Architecture">Search Engine Architecture</a><ul><li><a href=#crawlers-and-crawl-control aria-label="Crawler(s) and Crawl Control">Crawler(s) and Crawl Control</a></li><li><a href=#indexer-module aria-label="Indexer Module">Indexer Module</a></li><li><a href=#querier-engine-module aria-label="Querier Engine Module">Querier Engine Module</a></li></ul></li><li><a href=#implementation aria-label=Implementation>Implementation</a><ul><li><a href=#crawler-implementation aria-label="Crawler Implementation">Crawler Implementation</a></li><li><a href=#indexer-implementation aria-label="Indexer Implementation">Indexer Implementation</a></li><li><a href=#querier-implementation aria-label="Querier Implementation">Querier Implementation</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=search-engine-architecture>Search Engine Architecture<a hidden class=anchor aria-hidden=true href=#search-engine-architecture>#</a></h2><p>The architecture is based off of a 2001 paper (Searching the Web) by Arasu et al. published by the Association for Computing Machinery</p><figure><img loading=lazy src=/projects/tse/architecture.png alt="A schematic describing different components of the search engine design (from Arasu et al.)"><figcaption><p>A schematic describing different components of the search engine design (from Arasu et al.)</p></figcaption></figure><h3 id=crawlers-and-crawl-control>Crawler(s) and Crawl Control<a hidden class=anchor aria-hidden=true href=#crawlers-and-crawl-control>#</a></h3><p>The crawler(s) browse(s) the web for the search engine, similar to how a human would follow links to reach different pages.
The crawler starts at a specific <em>root</em> URL and starts the recursive process of extracting hyperlinks crawling these additional webpages.
The crawler caches each retrieved page into a <em>page repository</em>.
The crawler will continue visiting webpages until local resources are exhausted (i.e. no more storage in the page repository.</p><h3 id=indexer-module>Indexer Module<a hidden class=anchor aria-hidden=true href=#indexer-module>#</a></h3><p>The indexer module extracts all of the words from each page, and records the URL where the word occurred.
This process creates a large lookup table which provides all of the URLs that point to pages where a given word occurs.
Furthermore, the indexer also keeps track of the number of occurences <em>per page</em>, allowing for tracking pages with a large number of keywords on the single page.</p><h3 id=querier-engine-module>Querier Engine Module<a hidden class=anchor aria-hidden=true href=#querier-engine-module>#</a></h3><p>This module is responsible for receiving and fulfilling search requests from users.
It uses the page indexes to get a list of URLs which contain the keywords in the query.
Then the module ranks pages based on the number of occurrences of keywords in the user query.</p><h2 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h2><p>See <a href=https://github.com/srb-private-org/tiny-search-engine target=_blank>GitHub Repository</a>
for the source code.</p><h3 id=crawler-implementation>Crawler Implementation<a hidden class=anchor aria-hidden=true href=#crawler-implementation>#</a></h3><p>In this implementation, we use a single crawler to browse the webpages.
The implementation is a standalone program which crawls the web starting from the <em>seed</em> url, fetches links from webpages continuing to a certain <em>depth</em>, and then caches these webpages in a specific <em>page directory</em>.
The crawler ignores duplicate webpages when it scans each page for URLs.
Furthermore, it has guardrails in-place to prevent it from crawling the entirety of the web (we don&rsquo;t want to crawl pages that didn&rsquo;t give us permission ;) )</p><p>Usage:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ ./crawler seedURL pageDirectory maxDepth
</span></span></code></pre></div><p>Here we see the verbose output of the program:</p><figure><img loading=lazy src=/projects/tse/crawler.gif alt="The Crawler in Action"><figcaption><p>The Crawler in Action</p></figcaption></figure><h3 id=indexer-implementation>Indexer Implementation<a hidden class=anchor aria-hidden=true href=#indexer-implementation>#</a></h3><p>The indexer module produces an index file which stores the lookup table mapping each keyword to their host URLs with the number of ocurrences.
It browses the page directory created by the crawler and uses this directory to construct the index.
The module uses one main data structure: an <code>index</code> structure which maps from word to <code>(docID, #occurences)</code> pairs.
The documentID is a unique ID for each webpage cached in the pageDirectory.</p><p>Usage:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>./indexer pageDirectory indexFilename
</span></span></code></pre></div><h3 id=querier-implementation>Querier Implementation<a hidden class=anchor aria-hidden=true href=#querier-implementation>#</a></h3><p>The querier module is another standalone program which returns a page ranking according to a user&rsquo;s query in <code>stdin</code>.
To use the querier program, it simply requires the index file (created by the <code>indexer</code> program) and the page directory (created by the <code>crawler</code> program).
It reads the index file into memory as an <code>index</code> data structure.
The program parses each query and uses the union and intersection keywords (&lsquo;AND&rsquo; vs &lsquo;OR&rsquo;) in the query and uses the index to return a ranked list of URLs based on each score.</p><p>Usage:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>./querier pageDirectory indexFilename
</span></span></code></pre></div><figure><img loading=lazy src=/projects/tse/querier.gif alt="Querier in Action"><figcaption><p>Querier in Action</p></figcaption></figure></div><footer class=post-footer><ul class=post-tags><li><a href=https://srbarton43.github.io/tags/search-engine/>Search Engine</a></li><li><a href=https://srbarton43.github.io/tags/software/>Software</a></li><li><a href=https://srbarton43.github.io/tags/c/>C</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://srbarton43.github.io/>Samuel Barton</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>